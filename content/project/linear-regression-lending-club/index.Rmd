---
title: "Linear Regression to Predict Interest Rate"
date: "`r Sys.Date()`"
authors:
  - admin

tags:
  - ML
  
summary: "This is a short intro to linear regression with exampls inspired by my Data Science for Business class at LBS."

image:
  caption: ""
  focal_point: ""
  preview_only: true
  
  html_document:
    theme: cerulean
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Intro

This is a short intro to linear regression with exampls inspired by my Data Science for Business class at LBS.


## Libraries used within the code

```{r, load_libraries, include = TRUE, message=FALSE}
library(tidyverse) # the usual stuff: dplyr, readr, and other goodies
library(lubridate) # to handle dates
library(GGally) # for correlation-scatterplot matrix
library(car) # vif() function to check for multicolinearity
library(ggfortify) # to produce residual diagnostic plots 
library(rsample) # to split dataframe in training- & testing sets
library(here) # to read files and organise data
library(janitor) # clean_names()
library(broom) # use broom:augment() to get tidy table with regression output, residuals, etc
library(huxtable) # to get summary table of all models produced
library(caret) # to train more advanced models (k-fold cross-validation, stepwise regression, LASSO)
library(zoo) #to allow for time series operations
library(here)
library (usmap)

```

# Load and prepare the data

We start by loading the data to R in a dataframe.

```{r, load_data, warning=FALSE, message=FALSE}

lc_raw <- read_csv(here::here("csv","LendingClub Data.csv"),  skip=1) %>%  #since the first row is a title we want to skip it. 
  clean_names() # use janitor::clean_names()
```

# ICE the data: Inspect, Clean, Explore

Any data science engagement starts with ICE. Inspect, Clean and Explore the data.

```{r}
glimpse(lc_raw) 

lc_clean<- lc_raw %>%
  dplyr::select(-x20:-x80) %>% #delete empty columns
  filter(!is.na(int_rate)) %>%   #delete empty rows
  mutate(
    issue_d = mdy(issue_d),  # lubridate::mdy() to fix date format
    term = factor(term_months),     # turn 'term' into a categorical variable
    delinq_2yrs = factor(delinq_2yrs) # turn 'delinq_2yrs' into a categorical variable
  ) %>% 
  dplyr::select(-emp_title,-installment, -term_months, everything()) #move some not-so-important variables to the end. 


glimpse(lc_clean) 

    
```

The data is now in a clean format stored in the dataframe "lc_clean."

# Explore the Data through Visualisation

```{r, data_visualisation, message=FALSE}

# histogram of Interest Rates
lc_clean%>%
  ggplot(aes(int_rate))+
    geom_histogram()+
    theme_bw()+
    labs(title = "Histogram of Interest Rates",
         x = "interest rate",
         y = "count")


# Histogram of interest rates but using different color for loans of different grades 
lc_clean%>%
  ggplot(aes(int_rate, fill = grade))+
    geom_histogram()+
    theme_bw()+
    labs(title = "Histogram of Interest Rates by Loan Grade",
         x = "interest rate",
         y = "count")

# Scatter plot of loan amount against interest rate and add visually the line of best fit

lc_clean%>%
  ggplot(aes(y = int_rate, x = loan_amnt))+
    geom_point()+
    geom_smooth(method = "lm")+
    theme_bw()+
    labs(title = "Loan amount vs Interest rate",
         y = "interest rate",
         x = "loan amount")

# Scatter plot of annual income against interest rate and add visually the line of best fit 

lc_clean%>%
  ggplot(aes(y = int_rate, x = annual_inc))+
    geom_point()+
    geom_smooth(method = "lm", se = F)+
    theme_bw()+
    labs(title = "Annual Income vs Interest rate",
         y = "interest rate",
         x = "Annual Income")
  # scale_x_log10()

# In the same axes, produce box plots of the interest rate for every value of delinquencies
lc_clean%>%
  ggplot(aes(y = int_rate, x = delinq_2yrs))+
    geom_boxplot()+
    theme_bw()+
    labs(title = "Interest rates by Deliquency",
         y = "interest rate",
         x = "deliquency")

# Interest Rate over time

lc_clean%>%
  ggplot(aes(x= issue_d, y = int_rate))+
  geom_point()+
  geom_smooth()+
  theme_bw()+
  labs(title = "Interest Rates over time",
       x = "date",
       y = "interest rate")

# Interest rate over time by grade

lc_clean%>%
  ggplot(aes(x= issue_d, y = int_rate))+
  geom_point()+
  geom_smooth()+
  theme_bw()+
  labs(title = "Interest Rates over time by Grade",
       x = "date",
       y = "interest rate")+
  facet_wrap(~grade)

map_us <- lc_clean%>%
  mutate(state = addr_state)%>%
  group_by(state)%>%
  summarise(med_annual_inc = median(annual_inc))

# US map with median annual income distribution by state
plot_usmap(regions = "state", data = map_us, values = "med_annual_inc") + 
  labs(title = "Median Annual income by State",
       subtitle = "") + 
  scale_fill_continuous(low = "white", 
                        high = "darkgreen", 
                        name = "Median Annual income", 
                        label = scales::comma)+
  theme(panel.background=element_blank(), legend.position = "right")

# Histogram of interest rate by home ownership
lc_clean%>%
  ggplot(aes(int_rate, fill = home_ownership))+
    geom_histogram()+
    theme_bw()+
    labs(title = "Histogram of Interest Rates by Home Ownership",
         x = "interest rate",
         y = "count")


```

# Estimate simple linear regression models

We start with a simple but quite powerful model.

```{r, simple regression, message=FALSE}
# Use the lm command to estimate a regression model with the following variables "loan_amnt",  "term", "dti", "annual_inc", and "grade"

model1<-lm(data = lc_clean, int_rate~loan_amnt+term+dti+annual_inc+grade)
summary(model1)

```

Let us consider the following Questions:

a.  Are all variables statistically significant?

b.  Interpret all the coefficients in the regression.

c.  How much explanatory power does the model have?

d.  How wide would the 95% confidence interval of any prediction based on this model be?


a.  Every variable except for annual_inc is significant because they have p values below 0.05
b.  Coefficient Interpretation:
  -   loan_amnt: When your loan amount increases by 1 then your interest rate will increase by $7.169*10^{-2}$. Or if the loan amount increases by $100$ interest rate increases by $7.169$.
  -   Term60: The default for term is 36 so if you have a loan term of 60 then your interest rate will increase by $3.608*10^{-3}$.
  -   Dti: When your dti increases by 1 then your interest rate will increase by $4.328*10^{5}$. Or if the dti increases by $100000$ interest rate increases by $4.328$.
  -   annual_inc: When your annual income increases by 1 then your interest rate will decreases by $9.734*10^{-10}$. Or if the annual income decreases by $10,000,000,000$ interest rate increases by $9.734$.
  -   GradeB: The default for Grade is A so if you have a grade of B then your interest rate will increase by $3.554*10^{-2}$.
  -   GradeC: The default for Grade is A so if you have a grade of C then your interest rate will increase by $6.016*10^{-2}$.
  -   GradeD: The default for Grade is A so if you have a grade of D then your interest rate will increase by $8.172*10^{-2}$.
  -   GradeE: The default for Grade is A so if you have a grade of E then your interest rate will increase by $9.999*10^{-2}$.
  -   GradeF: The default for Grade is A so if you have a grade of F then your interest rate will increase by $1.195*20^{-1}$.
  -   GradeG: The default for Grade is A so if you have a grade of G then your interest rate will increase by $1.355*10^{-1}$.

c.  This model has an $Adjusted R^2=0.9197$ which means that it is explaining about 92% of the variability in the data. This is a very strong model.
d.  The 95% prediction interval can be found doing $\pm 1.96*0.01056$. Giving us $\left[-0.0206976;+0.0206976 \right]\Rightarrow \left[ -2.06976 \%;+2.06976 \% \right]$

# Feature Engineering

Let's build progressively more complex models, with more features exploring how the model improves.

```{r, Feature Engineering, message=FALSE}
#Add to model 1 an interaction between loan amount and grade. Use the "var1*var2" notation to define an interaction term in the linear regression model. This will add the interaction and the individual variables to the model. 

model2 <- lm(data = lc_clean, int_rate~loan_amnt*grade+term+dti+annual_inc)
summary(model2)
#Add to the model you just created above the square and the cube of annual income. Use the poly(var_name,3) command as a variable in the linear regression model.  

model3 <- lm(data = lc_clean, int_rate~loan_amnt*grade+term+dti+poly(annual_inc,3))
summary(model3)
#Continuing with the previous model, instead of annual income as a continuous variable break it down into quartiles and use quartile dummy variables. You can do this with the following command. 
  
lc_clean <- lc_clean %>% 
  mutate(quartiles_annual_inc = as.factor(ntile(annual_inc, 4)))

model4 <-lm(data = lc_clean, int_rate~loan_amnt*grade+term+dti+quartiles_annual_inc)
summary(model4)  

#Compare the performance of these four models using the anova command
anova(model1, model2, model3, model4) # compare all

anova(model1, model2) # compare each model individually
anova(model2, model3)
anova(model3, model4)
anova(model2,model4)

# anova marks if there is a significant difference between any input model and the first model in the list.
  
```

## Let us Consider the Following questions:

a.  Which of the four models has the most explanatory power in sample?
b.  In model 2, how should the estimated coefficient of the interaction term between grade B and loan amount, be interpreted?
c.  The problem of multicollinearity describes situations in which one feature is correlated with other features (or with a linear combination of other features). If your goal is to use the model to make predictions, should you be concerned about multicollinearity? Why, or why not?

a.  Model 4 has the highest explanatory power as its adjusted R value is the highest.
b.  For a loan of grade B the loan each unit increase in loan ammount decreases the interest rate by 6.617e-8.
c.  It is not a problem for prediction, it only influences the explanatory value of the model.

# Out of sample testing

Let's check the predictive accuracy of model2 by holding out a subset of the data to use as a testing data set. This method is sometimes referred to as the hold-out method for out-of-sample testing.

# Comment and explain each row of the code in the chunk below.

```{r, out of sample testing}
# split the data in dataframe called "testing" and another one called  "training". The "training" dataframe should have 80% of the data and the "testing" dataframe 20%.
set.seed(124)
library(rms)
library(Metrics)

train_test_split <- initial_split(lc_clean, prop = 0.8)
training <- training(train_test_split)
testing <- testing(train_test_split)

# Fit model2 on the training set 
model2_training<-lm(int_rate ~ loan_amnt + term+ dti + annual_inc + grade +grade:loan_amnt, training)
# Calculate the RMSE of the model in the training set (in sample)
rmse_training<-sqrt(mean((residuals(model2_training))^2))
# Use the model to make predictions out of sample in the testing set
pred<-predict(model2_training,testing)
# Calculate the RMSE of the model in the testing set (out of sample)
rmse_testing<- Metrics::rmse(pred,testing$int_rate)

```

The the accuracy of the model when testing on the training and test set changes less then 1% which indicates a good model and no over-fitting. The set seed affects the final values as it determines the randomness at which the data is split into training and test set.

# k-fold cross validation

We can also do out of sample testing using the method of k-fold cross validation. Using the caret package this is easy.

```{r, k-fold cross validation}
#the method "cv" stands for cross validation. We re going to create 10 folds.  

control <- trainControl (
    method="cv",
    number=10,
    verboseIter=F) #by setting this to true the model will report its progress after each estimation

#we are going to train the model and report the results using k-fold cross validation

plsFit<-train(
    int_rate ~ loan_amnt + term+ dti + annual_inc + grade +grade:loan_amnt ,
    lc_clean,
   method = "lm",
    trControl = control
   )
  

summary(plsFit)

control <- trainControl (
    method="cv",
    number=5,
    verboseIter=F) #by setting this to true the model will report its progress after each estimation

#we are going to train the model and report the results using k-fold cross validation

plsFit<-train(
    int_rate ~ loan_amnt + term+ dti + annual_inc + grade +grade:loan_amnt ,
    lc_clean,
   method = "lm",
    trControl = control
   )
  

summary(plsFit)

control <- trainControl (
    method="cv",
    number=15,
    verboseIter=F) #by setting this to true the model will report its progress after each estimation

#we are going to train the model and report the results using k-fold cross validation

plsFit<-train(
    int_rate ~ loan_amnt + term+ dti + annual_inc + grade +grade:loan_amnt ,
    lc_clean,
   method = "lm",
    trControl = control
   )
  

summary(plsFit)

```

The cross-validation is likely to give a more robust model but in this case the rmse compared to the out of sample testing doesn't seem to be very different. The k validation performs the the out of sample testing k times on the dataset but it is computationally more intensive.

# Sample size estimation and learning curves

We can use the hold out method for out-of-sample testing to check if we have a sufficiently large sample to estimate the model reliably. The idea is to set aside some of the data as a testing set. From the remaining data draw progressively larger training sets and check how the performance of the model on the testing set changes. If the performance no longer improves with larger training sets we know we have a large enough sample. The code below does this. Examine it and run it with different random seeds.

```{r, learning curves}
#select a testing dataset (25% of all data)
set.seed(12)

train_test_split <- initial_split(lc_clean, prop = 0.75)
remaining <- training(train_test_split)
testing <- testing(train_test_split)

#We are now going to run 30 models starting from a tiny training set drawn from the training data and progressively increasing its size. The testing set remains the same in all iterations.

#initiating the model by setting some parameters to zero
rmse_sample <- 0
sample_size<-0
Rsq_sample<-0

for(i in 1:30) {
#from the remaining dataset select a smaller subset to training the data
set.seed(100)
sample

  learning_split <- initial_split(remaining, prop = i/200)
  training <- training(learning_split)
  sample_size[i]=nrow(training)
  
  #traing the model on the small dataset
  model3<-lm(int_rate ~ loan_amnt + term+ dti + annual_inc + grade + grade:loan_amnt, training)
  #test the performance of the model on the large testing dataset. This stays fixed for all iterations.
  pred<-predict(model3,testing)
  rmse_sample[i]<-RMSE(pred,testing$int_rate)
  Rsq_sample[i]<-R2(pred,testing$int_rate)
}
plot(sample_size,rmse_sample)
plot(sample_size,Rsq_sample)
```

A sample size of 2000 is enough above which there isn't significant improvement in the model. If we try to reduce prediction error further is to try automated feature selection or other ways to improve the model.

# Regularization using LASSO regression

If we are in the region of the learning curve where we do not have enough data, one option is to use a regularization method such as LASSO.

Let's try to estimate a large and complicated model (many interactions and polynomials) on a small training dataset using OLS regression and hold-out validation method.

```{r, OLS model overfitting}

#split the data in testing and training. The training test is really small.
set.seed(1234)
train_test_split <- initial_split(lc_clean, prop = 0.01)
training <- training(train_test_split)
testing <- testing(train_test_split)

model_lm<-lm(int_rate ~ poly(loan_amnt,3) + term+ dti + annual_inc + grade +grade:poly(loan_amnt,3):term +poly(loan_amnt,3):term +grade:term, training)
predictions <- predict(model_lm,testing)

# Model prediction performance
data.frame(
  RMSE = RMSE(predictions, testing$int_rate),
  Rsquare = R2(predictions, testing$int_rate)
)
```

Not surprisingly this model does not perform well -- as we knew form the learning curves we constructed for a simpler model we need a lot more data to estimate this model reliably. Try running it again with different seeds. The model's performance tends to be sensitive to the choice of the training set.

LASSO regression offers one solution -- it extends the OLS regression by penalizing the model for setting any coefficient estimate to a value that is different from zero. The penalty is proportional to a parameter $\lambda$. This parameter cannot be estimated directly (and for this reason sometimes it is referred to as hyperparameter). $\lambda$ will be selected through k-fold cross validation so as to provide the best out-of-sample performance. As a result of the LASSO procedure, only those features that are more strongly associated with the outcome will have non-zero coefficient estimates and the estimated model will be less sensitive to the training set. Sometimes LASSO regression is referred to as regularization.

```{r, LASSO compared to OLS, warning=FALSE, message=FALSE}
# we will look for the optimal lambda in this sequence (we will try 1000 different lambdas, feel free to try more if necessary)
set.seed(1234)
lambda_seq <- seq(0, 0.01, length = 1000)

# lasso regression using k-fold cross validation to select the best lambda

lasso <- train(
 int_rate ~ poly(loan_amnt,3) + term+ dti + annual_inc + grade +grade:poly(loan_amnt,3):term +poly(loan_amnt,3):term +grade:term,
 data = training,
 method = "glmnet",
  preProc = c("center", "scale"), #This option standardizes the data before running the LASSO regression
  trControl = control,
  tuneGrid = expand.grid(alpha = 1, lambda = lambda_seq) #alpha=1 specifies to run a LASSO regression. If alpha=0 the model would run ridge regression.
  )


# Model coefficients
coef(lasso$finalModel, lasso$bestTune$lambda)

# Best lambda
lasso$bestTune$lambda

# Count of how many coefficients are greater than zero and how many are equal to zero

sum(coef(lasso$finalModel, lasso$bestTune$lambda)!=0)
sum(coef(lasso$finalModel, lasso$bestTune$lambda)==0)

# Make predictions
predictions <- predict(lasso,testing)

# Model prediction performance

data.frame(
  RMSE = RMSE(predictions, testing$int_rate),
  Rsquare = R2(predictions, testing$int_rate)
)

```

## Let us consider the following quesitons:

a.  Which model performs best out of sample, OLS regression or LASSO? Why?
b.  What value of lambda offers best performance? Is this sensitive to the random seed? Why?
c.  How many coefficients are zero and how many are non-zero in the LASSO model of best fit? Is number of zero (or non-zero) coefficients sensitive on the random seed? Why?
d.  Why is it important to standardize continuous variables before running LASSO?

a.  Lasso performs better because it better avoids overfitting the data then the OLS.
b.  $\lambda = 0.0003103103$ performs the best and this is sensitive to the random seed because the k-fold validation splits change based on the randomness, which then influence the model development to which the lambda is applied.
c.  28 coefficients are equal to 0 30 coefficients are not 0 this is influenced by the random seed as well becasue lambda changes the with the random seed which changes the penalty on each coefficient.
d.  To bring all variables to the same scale as they might be different units.

# Using Time Information

Let's try to further improve the model's predictive performance. So far we have not used any time series information. Effectively, all things being equal, our prediction for the interest rate of a loan given in 2009 would be the same as that of a loan given in 2011. Is this a good assumption?

First, investigate graphically whether there are any time trends in the interest rates. (Note that the variable "issue_d" only has information on the month the loan was awarded but not the exact date.) Can you use this information to further improve the forecasting accuracy of your model? Try controlling for time in a linear fashion (i.e., a linear time trend) and controlling for time as quarter-year dummies (this is a method to capture non-linear effects of time -- we assume that the impact of time doesn't change within a quarter but it can chance from quarter to quarter). Finally, check if time affect loans of different grades differently.

```{r, time trends}

#linear time trend (add code below)
lc_clean%>%
  ggplot(aes(x= issue_d, y = int_rate))+
  geom_point()+
  geom_smooth()+
  theme_bw()+
  labs(title = "Interest Rates over time",
       x = "date",
       y = "interest rate")

#linear time trend by grade (add code below)
lc_clean%>%
  ggplot(aes(x= issue_d, y = int_rate))+
  geom_point()+
  geom_smooth()+
  theme_bw()+
  labs(title = "Interest Rates over time by Grade",
       x = "date",
       y = "interest rate")+
  facet_wrap(~grade)

#Train models using OLS regression and k-fold cross-validation
#The first model has some explanatory variables and a linear time trend

time1<-train(
  int_rate ~ loan_amnt + term+ dti + annual_inc + grade +grade:loan_amnt+ issue_d,#fill your variables here "+ issue_d"
  lc_clean,
  method = "lm",
  trControl = control)

summary(time1)

#The second model has a different linear time trend for each grade class
time2<-train(
    int_rate ~ loan_amnt + term+ dti + annual_inc+grade:loan_amnt+ issue_d*grade, #fill your variables here 
    lc_clean,
   method = "lm",
    trControl = control
   )
  

summary(time2)

#Change the time trend to a quarter dummy variables.
#zoo::as.yearqrt() creates quarter dummies 
lc_clean_quarter<-lc_clean %>%
  mutate(yq = as.factor(as.yearqtr(lc_clean$issue_d, format = "%Y-%m-%d")))



time3<-train(
    int_rate ~ loan_amnt + term+ dti + annual_inc+grade*loan_amnt+ yq,#fill your variables here 
    lc_clean_quarter,
     method = "lm",
    trControl = control
   )
  
summary(time3)

#We specify one quarter dummy variable for each grade. This is going to be a large model as there are 19 quarters x 7 grades = 133 quarter-grade dummies.
time4<-train(
    int_rate ~ loan_amnt + term+ dti + annual_inc+grade*loan_amnt+ yq*grade ,#fill your variables here 
    lc_clean_quarter,
     method = "lm",
    trControl = control
   )

summary(time4)

data.frame(
  time1$results$RMSE,
  time2$results$RMSE,
  time3$results$RMSE,
  time4$results$RMSE)


```

Interest rate changes over time, but it changes at a different rates based on the grade of the loan. This means each step in the above exercise improves the prediction, with the best model being produced using year quarters in relation to loan grade.

# Using Bond Yields

One concern with using time trends for forecasting is that in order to make predictions for future loans we will need to project trends to the future. This is an extrapolation that may not be reasonable, especially if macroeconomic conditions in the future change. Furthermore, if we are using quarter-year dummies, it is not even possible to estimate the coefficient of these dummy variables for future quarters.

Instead, perhaps it's better to find the reasons as to why different periods are different from one another. The csv file "MonthBondYields.csv" contains information on the yield of US Treasuries on the first day of each month. Can you use it to see if you can improve your predictions without using time dummies?

```{r, bond yields}
#load the data to memory as a dataframe
bond_prices<-readr::read_csv(here::here("csv","MonthBondYields.csv"))

#make the date of the bond file comparable to the lending club dataset
#for some regional date/number (locale) settings this may not work. If it does try running the following line of code in the Console
#Sys.setlocale("LC_TIME","English")
bond_prices <- bond_prices %>%
  mutate(Date2=as.Date(paste("01",Date,sep="-"),"%d-%b-%y")) %>%
  select(-starts_with("X"))

#let's see what happened to bond yields over time. Lower bond yields mean the cost of borrowing has gone down.

bond_prices %>%
  ggplot(aes(x=Date2, y=Price))+geom_point(size=0.1, alpha=0.5)

library(janitor)

bond_prices<-bond_prices%>%
  clean_names()
bond_prices<-bond_prices%>%
  mutate(change = as.numeric(sub("%","",bond_prices$change_percent))/100)

#join the data using a left join
lc_with_bonds<-lc_clean %>%
  left_join(bond_prices, by = c("issue_d" = "date2")) %>%
  arrange(issue_d) %>%
  filter(!is.na(price))
  

  


# investigate graphically if there is a relationship 
lc_with_bonds%>%
  ggplot(aes(x=int_rate, y= price))+
  geom_point(size=0.1, alpha=0.5)+geom_smooth(method="lm")+
  labs(title = "Correlation of Interest Rate and Bond Prices",
       x = "Interest Rate",
       y = "Bond Price")+
  theme_bw()


lc_with_bonds%>%
  ggplot(aes(x=int_rate, y=price, color=grade))+
  geom_point(size=0.1, alpha=0.5)+geom_smooth(method="lm")+
  labs(title = "Correlation of Interest Rate and Bond Prices",
       subtitle = "Grouped By Grade",
       x = "Interest Rate",
       y = "Bond Price")+
  theme_bw()
  

#let's train a model using the bond information


plsFit<-train(
    int_rate ~ loan_amnt + term+ dti + annual_inc+grade*loan_amnt+grade*change , #fill your variables here 
    lc_with_bonds,
   method = "lm",
    trControl = control
   )
summary(plsFit)
```

## Do bond yields have any explanatory power?

The bond yields have explenatory power, however they do not improve the prediction to the same level as time dummies. The bond yields are correlated with interest rate to a reasonable level this allows the bond price to act in a similar way the time data does within the model, but less precisly, however bond prices can be extrapolated to the future so it is a better data to use if the model needs to predict future interest rates. 

# Further investigating model options

```{r}

#the method "cv" stands for cross validation. We re going to create 10 folds.  

control <- trainControl (
    method="cv",
    number=10,
    verboseIter=F) #by setting this to true the model will report its progress after each estimation

#we are going to train the model and report the results using k-fold cross validation

impr_model1<-train(
    int_rate ~ installment*poly(loan_amnt, 3) + annual_inc * term + dti + grade*price,
    lc_with_bonds,
   method = "lm",
    trControl = control
   )
  

summary(impr_model1)

impr_model2<-train(
    int_rate ~ installment*poly(loan_amnt, 3) + annual_inc * term + dti + grade*price + addr_state,
    lc_with_bonds,
   method = "lm",
    trControl = control
   )
  
summary(impr_model2)

impr_model3<-train(
    int_rate ~ installment*poly(loan_amnt, 3) + annual_inc * term + dti + grade*price + home_ownership,
    lc_with_bonds,
   method = "lm",
    trControl = control
   )

summary(impr_model3)

impr_model4<-train(
    int_rate ~ installment*poly(loan_amnt, 3) + annual_inc * term *home_ownership + dti + grade*price,
    lc_with_bonds,
   method = "lm",
    trControl = control
   )

print(summary(impr_model4))

impr_model5<-train(
    int_rate ~ installment*poly(loan_amnt, 3) + annual_inc * term + dti + grade*price+ home_ownership*addr_state,
    lc_with_bonds,
   method = "lm",
    trControl = control
   )

summary(impr_model5)

anova(impr_model1$finalModel, impr_model2$finalModel)
anova(impr_model2$finalModel, impr_model3$finalModel)
anova(impr_model3$finalModel, impr_model4$finalModel)
anova(impr_model4$finalModel, impr_model5$finalModel)

anova(impr_model1$finalModel,impr_model2$finalModel,impr_model3$finalModel,impr_model4$finalModel,impr_model5$finalModel)

```

```{r result = FALSE}

unique(lc_with_bonds$home_ownership)

set.seed(1234)
train_test_split <- initial_split(lc_with_bonds, prop = 0.5)
training <- training(train_test_split)
testing <- testing(train_test_split)

control <- trainControl (
    method="cv",
    number=10,
    verboseIter=F) #by setting this to true the model will report its progress after each estimation


lambda_seq <- seq(0, 0.01, length = 1000)

# lasso regression using k-fold cross validation to select the best lambda

lasso <- train(
 int_rate ~ installment*poly(loan_amnt,3) + term+ dti + grade*price +grade:poly(loan_amnt,3):term +poly(loan_amnt,3):term +grade:term+addr_state*annual_inc,
 data = training,
 method = "glmnet",
  preProc = c("center", "scale"), #This option standardizes the data before running the LASSO regression
  trControl = control,
  tuneGrid = expand.grid(alpha = 1, lambda = lambda_seq) #alpha=1 specifies to run a LASSO regression. If alpha=0 the model would run ridge regression.
  )


# Model coefficients
coef(lasso$finalModel, lasso$bestTune$lambda)

# Best lambda
lasso$bestTune$lambda

# Count of how many coefficients are greater than zero and how many are equal to zero

sum(coef(lasso$finalModel, lasso$bestTune$lambda)!=0)
sum(coef(lasso$finalModel, lasso$bestTune$lambda)==0)

# Make predictions
predictions <- predict(lasso,testing)

# Model prediction performance

data.frame(
  RMSE = RMSE(predictions, testing$int_rate),
  Rsquare = R2(predictions, testing$int_rate)
)


```

After trying out multiple models we conclude that model 4 has the lowest and highest R-squared but the improvements in predictive power are small at this point. Model 5th had a significant improvement at the 90% confidence level over model 4 but it also introduced a lot of non significant explanatory variables from the homeownership state interaction.
Between the Linear Models: The model 4 adjusted r-squared is 0.946. The residual standard error is 0.0086657, this means that the 95% confidence interval of a the prediction will contain $CI = \left[- 1.96*RSE; +1.96*RSE \right] = \left[-0.01698477;0.01698477 \right] = \left[-1.67%;1.67% \right]$ The features of model 4 can be seen within the model summary.
Using a LASSO regression results in much better preditive power of 98.75% and a confidence interval of $CI = \left[-0.81504%;0.81504% \right]$. For this model only 50% of the data was used for training as it could be seen from earlier training curves that is enough. The reason for the better performance of this model is likely to be attributed to the fact that LASSO drives the non-significant variables to zero which the other models can't thus retaining only the significant predictors from variable interactions.

## Using other publicly available datasets to further improve performance (e.g., quarterly data on US inflation or [CPI](https://fred.stlouisfed.org/series/CPALTT01USQ657N)).

```{r}
fed_raw <- read_csv(here::here("csv","CPALTT01USQ657N.csv")) %>%  #since the first row is a title we want to skip it. 
  clean_names() # use janitor::clean_names()

skimr::skim(fed_raw)
```

```{r}
fed_raw<-fed_raw%>%
  mutate(yq = as.factor(as.yearqtr(fed_raw$date, format = "%Y-%m-%d")))

lc_with_bonds <- lc_with_bonds%>%
  mutate(date = issue_d)%>%
  mutate(yq = as.factor(as.yearqtr(lc_with_bonds$issue_d, format = "%Y-%m-%d")))

lc_with_bonds_inf<-lc_with_bonds%>%
  left_join(fed_raw, by = "yq")%>%
  rename(inflation = cpaltt01usq657n)

```

```{r}

lc_with_bonds_inf%>%
  ggplot()+
  geom_point(aes(y = int_rate, x = inflation, color = grade))+
  geom_smooth(aes(y = int_rate, x = inflation, color = grade), method = "lm")+
  theme_bw()+
  labs(title = "Inflation vs Interest Rate",
       subtitle = "Grouped by Grades",
       x = "inflation",
       y = "interest rate")

```

```{r}

set.seed(1234)
train_test_split <- initial_split(lc_with_bonds, prop = 0.5)
training <- training(train_test_split)
testing <- testing(train_test_split)

control <- trainControl (
    method="cv",
    number=10,
    verboseIter=F) #by setting this to true the model will report its progress after each estimation


inf_model1<-train(
    int_rate ~ installment*poly(loan_amnt, 3) + annual_inc * term + dti + grade*price+ home_ownership*addr_state + grade * inflation,
    lc_with_bonds_inf,
   method = "lm",
    trControl = control
   )

summary(inf_model1)

anova(impr_model5$finalModel, inf_model1$finalModel)
```

```{r}
lc_with_bonds_inf<-lc_with_bonds_inf%>%
  filter(home_ownership != "NONE")



set.seed(1234)
train_test_split <- initial_split(lc_with_bonds_inf, prop = 0.5)
training <- training(train_test_split)
testing <- testing(train_test_split)

control <- trainControl (
    method="cv",
    number=10,
    verboseIter=F) #by setting this to true the model will report its progress after each estimation


lambda_seq <- seq(0, 0.01, length = 1000)

# lasso regression using k-fold cross validation to select the best lambda

lasso <- train(
 int_rate ~ installment*poly(loan_amnt,3) + term+ dti + grade*price +grade:poly(loan_amnt,3):term +poly(loan_amnt,3):term +grade:term+home_ownership*addr_state*annual_inc+inflation*grade*price,
 data = training,
 method = "glmnet",
  preProc = c("center", "scale"), #This option standardizes the data before running the LASSO regression
  trControl = control,
  tuneGrid = expand.grid(alpha = 1, lambda = lambda_seq) #alpha=1 specifies to run a LASSO regression. If alpha=0 the model would run ridge regression.
  )


# Model coefficients
coef(lasso$finalModel, lasso$bestTune$lambda)

# Best lambda
lasso$bestTune$lambda

# Count of how many coefficients are greater than zero and how many are equal to zero

sum(coef(lasso$finalModel, lasso$bestTune$lambda)!=0)
sum(coef(lasso$finalModel, lasso$bestTune$lambda)==0)

# Make predictions
predictions <- predict(lasso,testing)

# Model prediction performance

data.frame(
  RMSE = RMSE(predictions, testing$int_rate),
  Rsquare = R2(predictions, testing$int_rate)
)

```

## Does the Additional Data Make a Difference?

The interest rate is slightly correlated with inflation if segmented within grades. This means that adding inflation to the model helps produce a slight increase in prediction accuracy within both the linear model and the lasso regression, but it is not of a large amount as the correlation is weak.
